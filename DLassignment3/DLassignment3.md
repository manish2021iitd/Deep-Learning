Seq2Seq Transliteration with PyTorch Lightning
This repository contains code for a sequence-to-sequence (Seq2Seq) transliteration model implemented using PyTorch and PyTorch Lightning. The project includes two main models: a basic Seq2Seq model and an attention-enhanced Seq2Seq model. Both models are designed to handle character-level transliteration tasks, which involve converting words from one script to another.
Features
Seq2Seq model with configurable RNN, LSTM, or GRU cells.
Attention mechanism for improved performance.
Hyperparameter optimization using Weights & Biases (WandB) sweeps.
Training, validation, and testing on custom datasets.
Logging and monitoring with WandB.
Usage
Data Preparation


