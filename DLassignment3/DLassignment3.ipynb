{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhKfmMSHH8rsHIIjdVCQ7U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manish2021iitd/Deep-Learning/blob/main/DLassignment3/DLassignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CMvKI7MhvBxW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wandb\n",
        "from itertools import product\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, cell_type='rnn'):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Define the embedding layer\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "\n",
        "        # Choose the RNN cell type\n",
        "        if cell_type.lower() == 'rnn':\n",
        "            self.rnn = nn.RNN(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        elif cell_type.lower() == 'lstm':\n",
        "            self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        elif cell_type.lower() == 'gru':\n",
        "            self.rnn = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid cell type. Please choose from 'rnn', 'lstm', or 'gru'.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convert input to tensor\n",
        "        x = torch.tensor(x)\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        return output, hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, hidden_size, num_layers=1, cell_type='rnn'):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Define the embedding layer\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "\n",
        "        # Choose the RNN cell type\n",
        "        if cell_type.lower() == 'rnn':\n",
        "            self.rnn = nn.RNN(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        elif cell_type.lower() == 'lstm':\n",
        "            self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        elif cell_type.lower() == 'gru':\n",
        "            self.rnn = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid cell type. Please choose from 'rnn', 'lstm', or 'gru'.\")\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convert input to tensor\n",
        "        x = torch.tensor(x)\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        return output, hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target):\n",
        "        encoder_output, encoder_hidden = self.encoder(source)\n",
        "        decoder_output, _ = self.decoder(target, encoder_hidden)\n",
        "        return decoder_output\n",
        "\n",
        "# Example usage:\n",
        "input_size = 100  # Size of input vocabulary\n",
        "output_size = 100  # Size of output vocabulary\n",
        "input_embedding_size = 128\n",
        "hidden_size = 256\n",
        "num_layers = 1\n",
        "cell_type = 'lstm'\n",
        "\n",
        "encoder = Encoder(input_size, hidden_size, num_layers, cell_type)\n",
        "decoder = Decoder(output_size, hidden_size, num_layers, cell_type)\n",
        "model = Seq2Seq(encoder, decoder)\n",
        "\n",
        "# Print model architecture\n",
        "print(model)\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, input_data, target_data, input_vocab, target_vocab, max_length=None):\n",
        "        self.input_data = input_data\n",
        "        self.target_data = target_data\n",
        "        self.input_vocab = input_vocab\n",
        "        self.target_vocab = target_vocab\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_sequence = self.tokenize(self.input_data[idx], self.input_vocab)\n",
        "        target_sequence = self.tokenize(self.target_data[idx], self.target_vocab)\n",
        "\n",
        "        # Pad sequences to max_length\n",
        "        if self.max_length:\n",
        "            input_sequence = self.pad_sequence(input_sequence, self.max_length)\n",
        "            target_sequence = self.pad_sequence(target_sequence, self.max_length)\n",
        "\n",
        "        return input_sequence, target_sequence\n",
        "\n",
        "    def tokenize(self, sequence, vocab):\n",
        "        return [vocab.get(token, vocab['<unk>']) for token in sequence.split()]\n",
        "\n",
        "    def pad_sequence(self, sequence, max_length):\n",
        "        if len(sequence) < max_length:\n",
        "            sequence += [self.input_vocab['<pad>']] * (max_length - len(sequence))\n",
        "        return sequence\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def load_data(path):\n",
        "    df =pd.read_csv(path, header=None, names=['English', 'Hindi' ] )\n",
        "    return df['English'].tolist(), df['Hindi'].tolist()\n",
        "\n",
        "def create_vocab(text):\n",
        "    vocab = set(char for sentence in text for char in sentence)\n",
        "    vocab.add('<pad>')\n",
        "    vocab.add('<sos>')  # Start of sequence token\n",
        "    vocab.add('<eos>')  # End of sequence token\n",
        "    vocab.add('<unk>')  # Unknown token\n",
        "    # Create a dictionary mapping tokens to indices\n",
        "    token_to_idx = {token: idx for idx, token in enumerate(vocab)}\n",
        "    return token_to_idx\n",
        "\n",
        "\n",
        "\n",
        "# Define the paths to your CSV files\n",
        "train_data_file = '/kaggle/input/akasharantar/aksharantar_sampled/hin/hin_train.csv'\n",
        "val_data_file = '/kaggle/input/akasharantar/aksharantar_sampled/hin/hin_valid.csv'\n",
        "test_data_file = '/kaggle/input/akasharantar/aksharantar_sampled/hin/hin_test.csv'\n",
        "\n",
        "# Load datasets\n",
        "train_input_data, train_target_data = load_data(train_data_file)\n",
        "print(type(train_input_data))\n",
        "train_input_vocab = create_vocab(train_input_data)\n",
        "train_target_vocab =create_vocab(train_target_data)\n",
        "\n",
        "val_input_data, val_target_data = load_data(val_data_file)\n",
        "val_input_vocab = create_vocab(val_input_data)\n",
        "val_target_vocab =create_vocab(val_target_data)\n",
        "\n",
        "\n",
        "test_input_data, test_target_data = load_data(test_data_file)\n",
        "test_input_vocab = create_vocab(test_input_data)\n",
        "test_target_vocab =create_vocab(test_target_data)\n",
        "\n",
        "max_sequence_length = 50  # Example value, adjust as needed\n",
        "train_dataset = CustomDataset(train_input_data, train_target_data, train_input_vocab, train_target_vocab, max_length=max_sequence_length)\n",
        "val_dataset = CustomDataset(val_input_data, val_target_data, val_input_vocab, val_target_vocab, max_length=max_sequence_length)\n",
        "test_dataset = CustomDataset(test_input_data, test_target_data, test_input_vocab, test_target_vocab, max_length=max_sequence_length)\n",
        "\n",
        "# Create data loaders for each dataset\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define your training function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for batch_idx, data in enumerate(train_loader):\n",
        "            input_data, target_data = data\n",
        "            # Convert input and target sequences to tensors\n",
        "            input_data = torch.tensor(input_data)\n",
        "            target_data = torch.tensor(target_data)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input_data, target_data)\n",
        "            loss = criterion(output.view(-1, output.shape[-1]), target_data.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for data in val_loader:\n",
        "                input_data, target_data = data\n",
        "                output = model(input_data, target_data)\n",
        "                loss = criterion(output.view(-1, output.shape[-1]), target_data.view(-1))\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "        # Log metrics to wandb\n",
        "        # wandb.log({\"epoch\": epoch+1, \"train_loss\": avg_train_loss, \"val_loss\": avg_val_loss})\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "input_size = 100  # Size of input vocabulary\n",
        "output_size = 100  # Size of output vocabulary\n",
        "input_embedding_size = 128\n",
        "hidden_size = 256\n",
        "num_layers = 1\n",
        "cell_type = 'lstm'\n",
        "\n",
        "encoder = Encoder(input_size, hidden_size, num_layers, cell_type)\n",
        "decoder = Decoder(output_size, hidden_size, num_layers, cell_type)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "# Train the model\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)\n"
      ]
    }
  ]
}