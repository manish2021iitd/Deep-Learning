{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5f58ya9HqT7aCA0RycSFA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manish2021iitd/Deep-Learning/blob/main/DLassignment3/DLassignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMvKI7MhvBxW"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchtext pytorch-lightning wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wandb\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "wandb.login(key='e3c892d4f8c9cd9b9043d31938ad090f0a32cec1')\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, data, char2idx, idx2char, max_len=30):\n",
        "        self.data = data\n",
        "        self.char2idx = char2idx\n",
        "        self.idx2char = idx2char\n",
        "        self.max_len = max_len\n",
        "        self.unk_token = '<unk>'\n",
        "        self.unk_idx = len(char2idx)  # Assign unique index for unknown token\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_word, tgt_word = self.data[idx]\n",
        "\n",
        "        src_indices = [self.char2idx.get(char, self.unk_idx) for char in src_word]  # Use get() method to handle unknown characters\n",
        "        tgt_indices = [self.char2idx.get(char, self.unk_idx) for char in tgt_word]\n",
        "\n",
        "        src_indices = src_indices[:self.max_len]  # Truncate if longer than max_len\n",
        "        tgt_indices = tgt_indices[:self.max_len]  # Truncate if longer than max_len\n",
        "\n",
        "        src_indices += [self.char2idx['<pad>']] * (self.max_len - len(src_indices))  # Ensure padding\n",
        "        tgt_indices += [self.char2idx['<pad>']] * (self.max_len - len(tgt_indices))  # Ensure padding\n",
        "\n",
        "        return torch.LongTensor(src_indices), torch.LongTensor(tgt_indices)\n",
        "\n",
        "# Define the Seq2Seq model\n",
        "class Seq2SeqModel(pl.LightningModule):\n",
        "    def __init__(self, input_dim, output_dim, embed_dim, hidden_dim, n_layers, cell_type, bidirectional=False, dropout=0.3, learning_rate=0.001):\n",
        "        super(Seq2SeqModel, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        if cell_type == \"RNN\":\n",
        "            self.rnn = nn.RNN\n",
        "        elif cell_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM\n",
        "        elif cell_type == \"GRU\":\n",
        "            self.rnn = nn.GRU\n",
        "\n",
        "        self.encoder = nn.Embedding(input_dim, embed_dim)\n",
        "        self.encoder_rnn = self.rnn(embed_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True, bidirectional=bidirectional)\n",
        "\n",
        "        self.decoder = nn.Embedding(output_dim, embed_dim)\n",
        "        self.decoder_rnn = self.rnn(embed_dim, hidden_dim * (2 if bidirectional else 1), n_layers, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), output_dim)\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.size(0)\n",
        "        max_len = tgt.size(1)\n",
        "        tgt_vocab_size = self.hparams.output_dim\n",
        "\n",
        "        outputs = torch.zeros(batch_size, max_len, tgt_vocab_size).to(self.device)\n",
        "\n",
        "        embedded = self.encoder(src)\n",
        "        encoder_output, hidden = self.encoder_rnn(embedded)\n",
        "\n",
        "        if self.bidirectional:\n",
        "            hidden = self._concat_hidden_directions(hidden)\n",
        "\n",
        "        decoder_input = tgt[:, 0]\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            decoder_input = decoder_input.unsqueeze(1)\n",
        "            embedded = self.decoder(decoder_input)\n",
        "            decoder_output, hidden = self.decoder_rnn(embedded, hidden)\n",
        "            prediction = self.fc(decoder_output.squeeze(1))\n",
        "            outputs[:, t, :] = prediction\n",
        "\n",
        "            top1 = prediction.argmax(1)\n",
        "            decoder_input = tgt[:, t] if np.random.random() < teacher_forcing_ratio else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def _concat_hidden_directions(self, hidden):\n",
        "        if isinstance(hidden, tuple):  # LSTM hidden state is a tuple (hidden_state, cell_state)\n",
        "            hidden = (torch.cat((hidden[0][::2], hidden[0][1::2]), dim=2),\n",
        "                      torch.cat((hidden[1][::2], hidden[1][1::2]), dim=2))\n",
        "        else:  # GRU and RNN hidden state is a tensor\n",
        "            hidden = torch.cat((hidden[::2], hidden[1::2]), dim=2)\n",
        "        return hidden\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        src, tgt = batch\n",
        "        output = self(src, tgt)\n",
        "        loss = nn.CrossEntropyLoss()(output.view(-1, self.hparams.output_dim), tgt.view(-1))\n",
        "\n",
        "        preds = output.argmax(dim=2)\n",
        "        acc = calculate_accuracy(preds, tgt)\n",
        "\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
        "        self.log('train_acc', acc, on_step=True, on_epoch=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        src, tgt = batch\n",
        "        output = self(src, tgt, 0)\n",
        "\n",
        "        #print(f\"Output shape: {output.shape}\")  #Debugging\n",
        "        #print(f\"Target shape: {tgt.shape}\")  #Debugging\n",
        "\n",
        "        loss = nn.CrossEntropyLoss()(output.view(-1, self.hparams.output_dim), tgt.view(-1))\n",
        "\n",
        "        preds = output.argmax(dim=2)\n",
        "        acc = calculate_accuracy(preds, tgt)\n",
        "\n",
        "        self.log('val_loss', loss, on_step=True, on_epoch=True)\n",
        "        self.log('val_acc', acc, on_step=True, on_epoch=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        src, tgt = batch\n",
        "        output = self(src, tgt, 0)\n",
        "        loss = nn.CrossEntropyLoss()(output.view(-1, self.hparams.output_dim), tgt.view(-1))\n",
        "\n",
        "        preds = output.argmax(dim=2)\n",
        "        acc = calculate_accuracy(preds, tgt)\n",
        "\n",
        "        self.log('test_loss', loss, on_step=True, on_epoch=True)\n",
        "        self.log('test_acc', acc, on_step=True, on_epoch=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "\n",
        "# Define a function to calculate accuracy\n",
        "def calculate_accuracy(predictions, targets, pad_idx=0):\n",
        "    #print(f\"Predictions shape: {predictions.shape}\")  # Debug\n",
        "    #print(f\"Targets shape: {targets.shape}\")  # Debug\n",
        "\n",
        "    if len(predictions.shape) == 3:\n",
        "        _, pred_max = torch.max(predictions, 2)\n",
        "    elif len(predictions.shape) == 2:\n",
        "        pred_max = predictions\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected shape for predictions tensor\")\n",
        "\n",
        "    non_pad_elements = targets.ne(pad_idx)\n",
        "    correct = pred_max.eq(targets).masked_select(non_pad_elements).sum().item()\n",
        "    total = non_pad_elements.sum().item()\n",
        "    return correct / total if total > 0 else 0\n",
        "\n",
        "\n",
        "\n",
        "# Function to load data\n",
        "def load_data(file_path, max_len=30):\n",
        "    data = pd.read_csv(file_path, header=None)\n",
        "    data = [(str(row[0]), str(row[1])) for idx, row in data.iterrows()]\n",
        "    return data\n",
        "\n",
        "\n",
        "# Function to create character mappings\n",
        "def create_char_mappings(data):\n",
        "    chars = set()\n",
        "    for src, tgt in data:\n",
        "        chars.update(list(src))\n",
        "        chars.update(list(tgt))\n",
        "\n",
        "    char2idx = {char: idx + 1 for idx, char in enumerate(chars)}\n",
        "    char2idx['<pad>'] = 0\n",
        "    idx2char = {idx: char for char, idx in char2idx.items()}\n",
        "\n",
        "    return char2idx, idx2char\n",
        "\n",
        "\n",
        "def train_model(train_data_path, val_data_path, test_data_path, char2idx, idx2char, cell_type='LSTM', embed_dim=256, hidden_dim=512, n_layers=2, dropout=0.3, learning_rate=0.001, bidirectional=False, model_class=Seq2SeqModel):\n",
        "    train_dataset = TransliterationDataset(load_data(train_data_path), char2idx, idx2char)\n",
        "    val_dataset = TransliterationDataset(load_data(val_data_path), char2idx, idx2char)\n",
        "    test_dataset = TransliterationDataset(load_data(test_data_path), char2idx, idx2char)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,num_workers=3)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False,num_workers=3)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False,num_workers=3)\n",
        "\n",
        "    model = model_class(\n",
        "        input_dim=len(char2idx),\n",
        "        output_dim=len(char2idx),\n",
        "        embed_dim=embed_dim,\n",
        "        hidden_dim=hidden_dim,\n",
        "        n_layers=n_layers,\n",
        "        cell_type=cell_type,\n",
        "        bidirectional=bidirectional,\n",
        "        dropout=dropout,\n",
        "        learning_rate=learning_rate\n",
        "    )\n",
        "\n",
        "    wandb_logger = WandbLogger(project='seq2seq-transliteration')\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=10,\n",
        "        devices=1 if torch.cuda.is_available() else None,\n",
        "        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
        "        logger=wandb_logger,\n",
        "        log_every_n_steps=1\n",
        "    )\n",
        "\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "    trainer.test(model, test_loader)\n",
        "\n",
        "\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "    trainer.test(model, test_loader)\n",
        "\n",
        "\n",
        "def train_sweep():\n",
        "    with wandb.init() as run:\n",
        "        config = wandb.config\n",
        "\n",
        "        train_data_path = '/kaggle/input/akasharantar/aksharantar_sampled/hin/hin_train.csv'\n",
        "        val_data_path = '/kaggle/input/akasharantar/aksharantar_sampled/hin/hin_valid.csv'\n",
        "        test_data_path = '/kaggle/input/akasharantar/aksharantar_sampled/hin/hin_test.csv'\n",
        "\n",
        "        # Create character mappings\n",
        "        char2idx, idx2char = create_char_mappings(load_data(train_data_path))\n",
        "\n",
        "        # Train the model\n",
        "        train_model(\n",
        "            train_data_path=train_data_path,\n",
        "            val_data_path=val_data_path,\n",
        "            test_data_path=test_data_path,\n",
        "            char2idx=char2idx,\n",
        "            idx2char=idx2char,\n",
        "            cell_type=config.cell_type,\n",
        "            embed_dim=config.embed_dim,\n",
        "            hidden_dim=config.hidden_dim,\n",
        "            n_layers=config.n_layers,\n",
        "            dropout=config.dropout,\n",
        "            learning_rate=config.learning_rate,\n",
        "            bidirectional=config.bidirectional\n",
        "        )\n",
        "\n",
        "def run_hyperparameter_sweep():\n",
        "    sweep_config = {\n",
        "        'method': 'random',\n",
        "        'metric': {'name': 'val_loss', 'goal': 'minimize'},\n",
        "        'parameters': {\n",
        "            'embed_dim': {'values': [16, 32, 64, 128, 256]},\n",
        "            'hidden_dim': {'values': [16, 32, 64, 128, 256]},\n",
        "            'n_layers': {'values': [1, 2, 3]},\n",
        "            'dropout': {'values': [0.0, 0.1, 0.2, 0.3, 0.4]},\n",
        "            'learning_rate': {'values': [0.0001, 0.0005, 0.001, 0.005, 0.01]},\n",
        "            'bidirectional': {'values': [True, False]},\n",
        "            'cell_type': {'values': ['RNN', 'LSTM', 'GRU']}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Initialize the sweep\n",
        "    sweep_id = wandb.sweep(sweep_config, project='seq2seq-transliteration')\n",
        "\n",
        "    # Run the sweep\n",
        "    wandb.agent(sweep_id, train_sweep)\n",
        "\n",
        "# Run the hyperparameter sweep\n",
        "run_hyperparameter_sweep()\n"
      ]
    }
  ]
}