{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoGfAYd2jcG39epMhl/uH0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manish2021iitd/Deep-Learning/blob/main/DLassignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q1\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "#Fashion-MNIST dataset\n",
        "(training_images, training_labels), (testing_images,testing_labels) = fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.imshow(training_images[training_labels == i][0], cmap='gray')\n",
        "    plt.title(class_names[i])\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BpReo0LPBqaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2\n",
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        self.input_size = input_size #number of inputs nodes\n",
        "        self.hidden_sizes = hidden_sizes #list of neurons in each hidden layer\n",
        "        self.output_size = output_size #number of output neurons in output layer\n",
        "        self.weights = [] #list of weight matrices for each layer\n",
        "        self.biases = []  #list of bias vectors for each hidden layer and output layer\n",
        "\n",
        "        #innitializing the weights and biases for each layer\n",
        "        sizes = [input_size] + hidden_sizes + [output_size] #using list concatination to make a list of number of nodes in each layer\n",
        "        for i in range(len(sizes) - 1):\n",
        "            self.weights.append(np.random.randn(sizes[i+1], sizes[i]))\n",
        "            self.biases.append(np.random.randn(sizes[i+1]))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_scores = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "      # Forward pass\n",
        "      activations = [X]\n",
        "      for i in range(len(self.weights)):\n",
        "        z = np.dot(activations[-1], self.weights[i].T) + self.biases[i]\n",
        "        a = self.sigmoid(z) if i < len(self.weights) - 1 else self.softmax(z)\n",
        "        activations.append(a)\n",
        "      return activations\n",
        "\n",
        "    def predict(self, X):\n",
        "        #predict output probabilities\n",
        "        activations = self.forward(X)\n",
        "        return activations[-1]\n",
        "\n",
        "#load fashion-mnist dataset\n",
        "from keras.datasets import fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "#preprocess the data\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "#define neural network\n",
        "input_size = X_train.shape[1]  #28x28 images flattened\n",
        "hidden_sizes = [128, 64]  #size of hidden layers\n",
        "output_size = 10  # 10 classes in fashion-mnist\n",
        "\n",
        "#initialize neural network\n",
        "model = NeuralNetwork(input_size, hidden_sizes, output_size)\n",
        "\n",
        "#predict probabilities for the test set\n",
        "probabilities = model.predict(X_test)\n",
        "\n",
        "#print the output probabilities\n",
        "print(\"Output Probabilities:\", probabilities)"
      ],
      "metadata": {
        "id": "RiOXwxOdBpij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raq_icU4Ba9J"
      },
      "outputs": [],
      "source": [
        "#Q3\n",
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self,int_method, input_size,hidden_layers, hidden_sizes, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_sizes = hidden_sizes if isinstance(hidden_sizes, list) else [hidden_sizes]\n",
        "        self.output_size = output_size\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.dw = []\n",
        "        self.db = []\n",
        "        self.delta = []\n",
        "\n",
        "        sizes = []\n",
        "        if isinstance(hidden_layers, int):\n",
        "          for j in range(hidden_layers):\n",
        "            sizes.extend(hidden_sizes)\n",
        "        else:\n",
        "          sizes = hidden_layers\n",
        "        sizes = [input_size] + sizes + [output_size]\n",
        "\n",
        "        for i in range(len(sizes) - 1):\n",
        "            if int_method == \"Xavier\":\n",
        "                self.weights.append(np.random.randn(sizes[i], sizes[i+1]) * np.sqrt(1/self.hidden_sizes[i-1]))\n",
        "            else:\n",
        "                self.weights.append(np.random.randn(sizes[i], sizes[i+1]))\n",
        "            self.biases.append(np.random.randn(sizes[i+1], 1))\n",
        "        #initialize velocities for momentum-based optimization\n",
        "        self.velocities = [np.zeros_like(w) for w in self.weights]\n",
        "\n",
        "        #initialize rmsprop parameters\n",
        "        self.rmsprop_cache_w = [np.zeros_like(w) for w in self.weights]\n",
        "        self.rmsprop_cache_b = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "        #initialize adam parameters\n",
        "        self.adam_m_w = [np.zeros_like(w) for w in self.weights]\n",
        "        self.adam_m_b = [np.zeros_like(b) for b in self.biases]\n",
        "        self.adam_v_w = [np.zeros_like(w) for w in self.weights]\n",
        "        self.adam_v_b = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "      return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def relu(x):\n",
        "      return (x>0)*(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh(x):\n",
        "      return np.tanh(x)\n",
        "\n",
        "    def activation_fun(self,x, act_fun):\n",
        "      if act_fun == 'sigmoid':\n",
        "        return NeuralNetwork.sigmoid(x)\n",
        "      elif act_fun == 'tanh':\n",
        "        return NeuralNetwork.tanh(x)\n",
        "      elif act_fun == 'relu':\n",
        "        return NeuralNetwork.relu(x)\n",
        "\n",
        "    def der_activation(self,x,act_fun):\n",
        "      if act_fun == 'sigmoid':\n",
        "        return NeuralNetwork.sigmoid(x)*(1- NeuralNetwork.sigmoid(x))\n",
        "      elif act_fun == 'tanh':\n",
        "        return 1 - (np.tanh(x))**2\n",
        "      elif act_fun == 'relu':\n",
        "        return np.where(x <= 0, 0, 1)\n",
        "\n",
        "    def softmax(self,x):\n",
        "      exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "      return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def softmax_derivative(self,x):\n",
        "      softmax_output = self.softmax(x)\n",
        "      return softmax_output * (1 - softmax_output)\n",
        "\n",
        "    @staticmethod\n",
        "    def logloss(y, y_label):\n",
        "      error = -(y_label*np.log(y+1e-10)+(1-y_label)*np.log(1-y+1e-10))\n",
        "      return np.sum(error, axis = 0)/error.shape[0]\n",
        "\n",
        "\n",
        "    def Cross_entropy(self,y, y_hat):\n",
        "      epsilon = 1e-15  # to prevent log(0) which is undefined\n",
        "      y_hat = np.clip(y_hat, epsilon, 1 - epsilon)  # clip values to prevent log(0)\n",
        "      loss = -np.mean(np.sum(y.T * np.log(y_hat), axis=0))\n",
        "      return loss\n",
        "\n",
        "    #forwardpropagation\n",
        "    def forward(self, X, act_fun):\n",
        "      self.activations = [] #create a list of all activations\n",
        "      self.preactivations = []\n",
        "      self.activations.append(X)\n",
        "      self.preactivations.append(X)\n",
        "      for i in range(len(self.weights)):\n",
        "        z = np.dot(self.weights[i].T,self.activations[-1]) + self.biases[i]\n",
        "        self.preactivations.append(z)  #preactivation\n",
        "        if i < len(self.weights) - 1:\n",
        "          a = self.activation_fun(z,act_fun)\n",
        "        else:\n",
        "          a = self.softmax(z) #output function\n",
        "        #print(\"activation:\",i,a.shape)\n",
        "        self.activations.append(a) #appending the activations list\n",
        "      return self.activations , self.preactivations\n",
        "\n",
        "    #backwardpropagation\n",
        "    def backward(self, y,act_fun):\n",
        "\n",
        "        self.dw = []\n",
        "        self.db = []\n",
        "        self.delta = []\n",
        "        for i in range(len(self.weights) - 1, -1, -1):\n",
        "            if i == len(self.weights) - 1:\n",
        "                dz = (self.activations[-1] - y.T)\n",
        "            else:\n",
        "                dz = np.dot(self.weights[i+1],self.delta[-1]) *  (self.der_activation(self.preactivations[i+1],act_fun))\n",
        "            self.delta.append(dz)\n",
        "            dw_temp = np.dot(dz,self.activations[i].T)\n",
        "            self.dw.append(dw_temp)\n",
        "            db_temp = np.sum(dz, axis=0)\n",
        "            self.db.append(db_temp)\n",
        "\n",
        "        self.dw.reverse()\n",
        "        self.db.reverse()\n",
        "        return self.dw, self.db\n",
        "\n",
        "    def update(self, X, y,act_fun, optimizer, learning_rate=0.0001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        activations = self.forward(X,act_fun)\n",
        "        #print(activations)\n",
        "        dw, db = self.backward(y,act_fun)\n",
        "        #print(dw)\n",
        "        #print(db)\n",
        "        if optimizer == 'sgd':\n",
        "          for i in range(len(self.weights)):\n",
        "            #print(dw[i].shape)\n",
        "            #print(self.weights[i].shape)\n",
        "            self.weights[i] -= learning_rate * dw[i].T\n",
        "            #print(self.biases[i].shape)\n",
        "            #print(db[i].shape)\n",
        "            self.biases[i] -= learning_rate * np.mean(db[i], axis=0, keepdims=True)\n",
        "\n",
        "        elif optimizer == 'momentum':\n",
        "          for i in range(len(self.weights)):\n",
        "            self.velocities[i] = beta1 * self.velocities[i] + (1 - beta1) * dw[i].T\n",
        "            self.weights[i] -= learning_rate * self.velocities[i]\n",
        "            self.biases[i] -= learning_rate * np.mean(db[i], axis=0, keepdims=True)\n",
        "\n",
        "\n",
        "        elif optimizer == 'nesterov':\n",
        "          lookahead_weights = [w - beta1 * v for w, v in zip(self.weights, self.velocities)]\n",
        "          lookahead_biases = [b - beta1 * v[-1] for b, v in zip(self.biases, self.velocities)]\n",
        "          lookahead_activations = self.forward(X,act_fun)\n",
        "          lookahead_dw, lookahead_db = self.backward(y, lookahead_activations)\n",
        "          for i in range(len(self.weights)):\n",
        "            self.velocities[i] = beta1 * self.velocities[i] + (1 - beta1) * lookahead_dw[i].T\n",
        "            self.weights[i] -= learning_rate * self.velocities[i]\n",
        "            self.biases[i] -= learning_rate * np.mean(lookahead_db[i], axis=0, keepdims=True)\n",
        "\n",
        "        elif optimizer == 'rmsprop':\n",
        "          for i in range(len(self.weights)):\n",
        "            self.rmsprop_cache_w[i] = beta1 * self.rmsprop_cache_w[i] + (1 - beta1) * (dw[i].T)**2\n",
        "            self.rmsprop_cache_b[i] = beta1 * self.rmsprop_cache_b[i] + (1 - beta1) * np.mean(db[i], axis=0, keepdims=True)**2\n",
        "            self.weights[i] -= learning_rate * dw[i].T / (np.sqrt(self.rmsprop_cache_w[i]) + epsilon)\n",
        "            self.biases[i] -= learning_rate * np.mean(db[i], axis=0, keepdims=True) / (np.sqrt(self.rmsprop_cache_b[i]) + epsilon)\n",
        "\n",
        "        elif optimizer == 'adam':\n",
        "          for i in range(len(self.weights)):\n",
        "            self.adam_m_w[i] = beta1 * self.adam_m_w[i] + (1 - beta1) * dw[i].T\n",
        "            self.adam_m_b[i] = beta1 * self.adam_m_b[i] + (1 - beta1) * np.mean(db[i], axis=0, keepdims=True)\n",
        "            self.adam_v_w[i] = beta2 * self.adam_v_w[i] + (1 - beta2) * (dw[i].T)**2\n",
        "            self.adam_v_b[i] = beta2 * self.adam_v_b[i] + (1 - beta2) * np.mean(db[i], axis=0, keepdims=True)**2\n",
        "            m_w_hat = self.adam_m_w[i] / (1 - beta1**(i+1))\n",
        "            m_b_hat = self.adam_m_b[i] / (1 - beta1**(i+1))\n",
        "            v_w_hat = self.adam_v_w[i] / (1 - beta2**(i+1))\n",
        "            v_b_hat = self.adam_v_b[i] / (1 - beta2**(i+1))\n",
        "            self.weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + epsilon)\n",
        "            self.biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n",
        "\n",
        "        elif optimizer == 'nadam':\n",
        "          for i in range(len(self.weights)):\n",
        "            self.adam_m_w[i] = beta1 * self.adam_m_w[i] + (1 - beta1) * dw[i].T\n",
        "            self.adam_m_b[i] = beta1 * self.adam_m_b[i] + (1 - beta1) * np.mean(db[i], axis=0, keepdims=True)\n",
        "            self.adam_v_w[i] = beta2 * self.adam_v_w[i] + (1 - beta2) * dw[i].T**2\n",
        "            self.adam_v_b[i] = beta2 * self.adam_v_b[i] + (1 - beta2) * np.mean(db[i], axis=0, keepdims=True)**2\n",
        "            m_w_hat = self.adam_m_w[i] / (1 - beta1**(i+1))\n",
        "            m_b_hat = self.adam_m_b[i] / (1 - beta1**(i+1))\n",
        "            v_w_hat = self.adam_v_w[i] / (1 - beta2**(i+1))\n",
        "            v_b_hat = self.adam_v_b[i] / (1 - beta2**(i+1))\n",
        "            self.weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + epsilon)\n",
        "            self.biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n",
        "\n",
        "        return self.weights,self.biases\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def train(self, X, y,act_fun, optimizer, epochs, batch_size,learning_rate):\n",
        "      #train the neural network using different optimizers\n",
        "      for _ in range(epochs):\n",
        "        for i in range(0, len(X), batch_size):\n",
        "          X_batch = X[:,i:i+batch_size]\n",
        "          #print(\"X_batch:\",X_batch.shape)\n",
        "          y_batch = y[i:i+batch_size]\n",
        "          #print(\"y_batch:\",y_batch.shape)\n",
        "          self.weights,self.biases=self.update(X_batch,y_batch,act_fun,optimizer,learning_rate)\n",
        "        a,b = self.forward(X, act_fun)\n",
        "        y_hat = a[-1]\n",
        "        epoch_loss = self.Cross_entropy(y, y_hat)\n",
        "      return epoch_loss\n",
        "\n",
        "    def test(self, X,act_fun):\n",
        "      a,b = self.forward(X,act_fun)\n",
        "      return a[-1]\n",
        "\n",
        "\n",
        "#load fashion-mnist dataset\n",
        "from keras.datasets import fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "#preprocess the data\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "#print('X_tain:',X_train.shape)\n",
        "#print(\"X_test:\",X_test.shape)\n",
        "X_train = X_train.T\n",
        "#print(\"X_tain.T:\",X_train.shape)\n",
        "\n",
        "\n",
        "#define neural network\n",
        "int_method = ['random','xavier']\n",
        "input_size = 784#size of inputs\n",
        "hidden_layers=1\n",
        "hidden_sizes = [128]  #size of neurons in each hidden layers\n",
        "output_size = 10  #taking 10 output neurons as we have classify 10 classes in fashion-mnist\n",
        "\n",
        "#convert labels to one-hot encoding\n",
        "num_classes = 10\n",
        "y_train_one_hot = np.eye(num_classes)[y_train]\n",
        "y_test_one_hot = np.eye(num_classes)[y_test]\n",
        "#print(\"y_train_one_hot:\",y_train_one_hot.shape)\n",
        "#print(\"y_test_one_hot:\",y_test_one_hot.shape)\n",
        "\n",
        "#initialize neural network\n",
        "model = NeuralNetwork(int_method,input_size,hidden_layers, hidden_sizes, output_size)\n",
        "\n",
        "'''please choose act_fun= 'sigmoid'/'relu'/'tanh' ,optimizer = 'sgd'/'momentum'/'nesterov'/'rmsprop'/'adam'/'nadam'\n",
        "and number of epochs and batch size here'''\n",
        "model.train(X_train, y_train_one_hot,act_fun='sigmoid',optimizer='sgd', epochs=5, batch_size=32,learning_rate=0.01)\n",
        "results = model.test(X_test.T,'sigmoid')\n",
        "\n",
        "def compute_accuracy(results, y_test_one_hot):\n",
        "    results = np.argmax(results.T, axis=1)\n",
        "    real_labels = np.argmax(y_test_one_hot, axis=1)\n",
        "    accuracy = np.mean(results == real_labels)\n",
        "    return accuracy\n",
        "\n",
        "acc  = compute_accuracy(results, y_test_one_hot)\n",
        "print(\"The accuracy of testing is \", acc*100, \"%\")\n",
        "\n",
        "#References:\n",
        "#1.https://cs229.stanford.edu/main_notes.pdf\n",
        "#2.http://www.cse.iitm.ac.in/~miteshk/CS6910.html\n",
        "#3.https://visualstudiomagazine.com/Articles/2017/06/01/Back-Propagation.aspx?Page=2\n",
        "#4.https://medium.com/@ipylypenko/exploring-neural-networks-with-fashion-mnist-b0a8214b7b7b\n",
        "#5.https://www.youtube.com/watch?v=Wo5dMEP_BbI&list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3\n",
        "#7.https://www.youtube.com/watch?v=LQvRhQwDOm0\n",
        "#8.https://towardsdatascience.com/implementing-different-activation-functions-and-weight-initialization-methods-using-python-c78643b9f20f"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "rqLQLN2FBuBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import numpy as np\n",
        "from types import SimpleNamespace\n",
        "import random"
      ],
      "metadata": {
        "id": "97Khz7yjBz6F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key='e3c892d4f8c9cd9b9043d31938ad090f0a32cec1')"
      ],
      "metadata": {
        "id": "ZDf0n3tYB2wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    with wandb.init() as run:\n",
        "        config = wandb.config\n",
        "\n",
        "\n",
        "        hidden_sizes = [config.hidden_sizes] if isinstance(config.hidden_sizes, int) else config.hidden_sizes\n",
        "\n",
        "        model = NeuralNetwork(config.int_method, input_size, config.hidden_layers, hidden_sizes, output_size)\n",
        "\n",
        "\n",
        "        epoch_loss = model.train(X_train, y_train_one_hot, config.activation, config.optimizer,\n",
        "                                  config.epochs, config.batch_size, config.learning_rate)\n",
        "\n",
        "\n",
        "        results = model.test(X_test.T, config.activation)\n",
        "\n",
        "\n",
        "        accuracy = compute_accuracy(results, y_test_one_hot)\n",
        "\n",
        "\n",
        "        wandb.log({\"accuracy\": accuracy*100})\n",
        "        wandb.log({\"loss\": epoch_loss})\n",
        "\n",
        "#define the sweep configuration\n",
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'name': 'sweep_cross_entropy',\n",
        "    'metric': {\n",
        "        'name': 'accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {'values': [5, 10]},\n",
        "        'hidden_layers': {'values':[3, 4, 5]},\n",
        "        'hidden_sizes': {'values': [32, 64, 128]},\n",
        "        'activation': {'values': ['sigmoid', 'relu', 'tanh']},\n",
        "        'loss': {'values': ['cross_entropy']},\n",
        "        'weight_decay': {'values': [0, 0.0005, 0.5]},\n",
        "        'learning_rate': {'values': [0.0001, 0.00001]},\n",
        "        'optimizer': {'values': ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']},\n",
        "        'batch_size': {'values': [16, 32, 64]},\n",
        "        'int_method':{'values':['radnom','xavier']}\n",
        "    }\n",
        "}\n",
        "\n",
        "#the sweep\n",
        "sweep_id = wandb.sweep(sweep=sweep_config, project='DLassignment1')\n",
        "\n",
        "# Run the sweep agent\n",
        "wandb.agent(sweep_id, function=main,count=100)"
      ],
      "metadata": {
        "id": "n78SQe25B7Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bd6gEwcMB9ZI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}