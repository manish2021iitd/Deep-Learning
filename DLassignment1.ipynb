{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhWn7MfI89tcupSdu3WLTk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manish2021iitd/Deep-Learning/blob/main/DLassignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#wandB â€“ Install the W&B library\n",
        "!pip install wandb\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "#essentials libararies\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pylab as pl\n",
        "import random"
      ],
      "metadata": {
        "id": "9J0dLhYQzQF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dac2-PG4ZrG5"
      },
      "outputs": [],
      "source": [
        "wandb.login(key='e3c892d4f8c9cd9b9043d31938ad090f0a32cec1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ws9718y3ZrG5"
      },
      "outputs": [],
      "source": [
        "#Q1\n",
        "wandb.init(project='DLassignment1')\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "#Fashion-MNIST dataset\n",
        "(training_images, training_labels), (testing_images,testing_labels) = fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "images = [];\n",
        "labels = [];\n",
        "for i in range(54000):\n",
        "  if len(labels) >= 10:\n",
        "    break;\n",
        "  if class_names[training_labels[i]] not in labels:\n",
        "      images.append(training_images[i])\n",
        "      labels.append(class_names[training_labels[i]])\n",
        "wandb.log({\"examples\": [ wandb.Image(img, caption=caption) for img, caption in zip(images,labels)]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pj_dnpflZrG5"
      },
      "outputs": [],
      "source": [
        "#Q2\n",
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        self.input_size = input_size #number of inputs nodes\n",
        "        self.hidden_sizes = hidden_sizes #list of neurons in each hidden layer\n",
        "        self.output_size = output_size #number of output neurons in output layer\n",
        "        self.weights = [] #list of weight matrices for each layer\n",
        "        self.biases = []  #list of bias vectors for each hidden layer and output layer\n",
        "\n",
        "        #innitializing the weights and biases for each layer\n",
        "        sizes = [input_size] + hidden_sizes + [output_size] #using list concatination to make a list of number of nodes in each layer\n",
        "        for i in range(len(sizes) - 1):\n",
        "            self.weights.append(np.random.randn(sizes[i+1], sizes[i]))\n",
        "            self.biases.append(np.random.randn(sizes[i+1]))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_scores = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "      # Forward pass\n",
        "      activations = [X]\n",
        "      for i in range(len(self.weights)):\n",
        "        z = np.dot(activations[-1], self.weights[i].T) + self.biases[i]\n",
        "        a = self.sigmoid(z) if i < len(self.weights) - 1 else self.softmax(z)\n",
        "        activations.append(a)\n",
        "      return activations\n",
        "\n",
        "    def predict(self, X):\n",
        "        #predict output probabilities\n",
        "        activations = self.forward(X)\n",
        "        return activations[-1]\n",
        "\n",
        "#load fashion-mnist dataset\n",
        "from keras.datasets import fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "#preprocess the data\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "#define neural network\n",
        "input_size = X_train.shape[1]  #28x28 images flattened\n",
        "hidden_sizes = [128, 64]  #size of hidden layers\n",
        "output_size = 10  # 10 classes in fashion-mnist\n",
        "\n",
        "#initialize neural network\n",
        "model = NeuralNetwork(input_size, hidden_sizes, output_size)\n",
        "\n",
        "#predict probabilities for the test set\n",
        "probabilities = model.predict(X_test)\n",
        "\n",
        "#print the output probabilities\n",
        "print(\"Output Probabilities:\", probabilities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pLLxizoFZrG6"
      },
      "outputs": [],
      "source": [
        "#Q3\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_layers, hidden_sizes, output_size, int_method='random'):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_sizes = hidden_sizes if isinstance(hidden_sizes, list) else [hidden_sizes]\n",
        "        self.output_size = output_size\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        sizes = [input_size] + self.hidden_sizes + [output_size]\n",
        "\n",
        "        for i in range(len(sizes) - 1):\n",
        "            if int_method == \"Xavier\":\n",
        "                self.weights.append(np.random.randn(sizes[i], sizes[i+1]) * np.sqrt(1/sizes[i]))\n",
        "            else:\n",
        "                self.weights.append(np.random.randn(sizes[i], sizes[i+1]))\n",
        "            self.biases.append(np.random.randn(1, sizes[i+1]))\n",
        "\n",
        "        self.adam_v_b = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def relu(x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh(x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def MSE(self,y, y_hat, weight_decay,norm):\n",
        "      loss = np.mean((y-(y_hat.T))**2) + (weight_decay/2)*(norm)\n",
        "      return loss\n",
        "\n",
        "    def Cross_entropy(self, y, y_hat,weight_decay,norm):\n",
        "      #y_hat = np.clip(y_hat.reshape(-1, self.output_size), epsilon, 1 - epsilon)  # clip values to prevent log(0)\n",
        "      loss = -np.mean(y * np.log(y_hat.T)) + (weight_decay/2)*norm\n",
        "      return loss\n",
        "\n",
        "    def frobenius_norm(self,matrix):\n",
        "      return np.linalg.norm(matrix, ord='fro')\n",
        "\n",
        "\n",
        "    def evaluate(self, x, y,act_fun,weight_decay,loss_fun):\n",
        "        predictions = self.forward(x,act_fun)[-1]\n",
        "        predictions =np.array(predictions)\n",
        "        norm = 0\n",
        "        for i in range(len(self.weights)):\n",
        "          norm += self.frobenius_norm(self.weights[i])**2\n",
        "        if loss_fun == 'MSE':\n",
        "          loss = self.MSE(y.reshape(10, -1),predictions,weight_decay,norm)\n",
        "        else:\n",
        "          loss = self.Cross_entropy(y.reshape(10, -1),predictions,weight_decay,norm)\n",
        "        accuracy = np.mean(np.argmax(predictions, axis=1) == np.argmax(y, axis=1))\n",
        "        return loss, accuracy\n",
        "\n",
        "    def activation_fun(self, x, act_fun):\n",
        "        if act_fun == 'sigmoid':\n",
        "            return self.sigmoid(x)\n",
        "        elif act_fun == 'tanh':\n",
        "            return self.tanh(x)\n",
        "        elif act_fun == 'relu':\n",
        "            return self.relu(x)\n",
        "\n",
        "    def der_activation(self, x, act_fun):\n",
        "        if act_fun == 'sigmoid':\n",
        "            return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
        "        elif act_fun == 'tanh':\n",
        "            return 1. - np.tanh(x) ** 2\n",
        "        elif act_fun == 'relu':\n",
        "            return np.where(x <= 0, 0, 1)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, X, act_fun):\n",
        "        activations = [X]\n",
        "        for i in range(len(self.weights)):\n",
        "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
        "            if i < len(self.weights) - 1:\n",
        "                a = self.activation_fun(z, act_fun)\n",
        "            else:\n",
        "                a = self.softmax(z)\n",
        "            activations.append(a)\n",
        "        return activations\n",
        "\n",
        "    def backward(self, X, y, act_fun):\n",
        "        activations = self.forward(X, act_fun)\n",
        "        delta = activations[-1] - y\n",
        "        deltas = [delta]\n",
        "        for i in range(len(self.weights) - 1, 0, -1):\n",
        "            delta = np.dot(deltas[-1], self.weights[i].T) * self.der_activation(activations[i], act_fun)\n",
        "            deltas.append(delta)\n",
        "        deltas.reverse()\n",
        "        return deltas\n",
        "\n",
        "    def update(self, X, y, act_fun,optimizer, learning_rate, weight_decay,beta1 = 0.9,beta2=0.999,epsilon=1e-8):\n",
        "        activations = self.forward(X, act_fun)\n",
        "        deltas = self.backward(X, y, act_fun)\n",
        "\n",
        "        if optimizer == 'sgd':\n",
        "            for i in range(len(self.weights)):\n",
        "                self.weights[i] -= learning_rate * ((np.dot(activations[i].T, deltas[i]) + weight_decay * self.weights[i]))\n",
        "                self.biases[i] -= learning_rate * np.mean(deltas[i], axis=0)\n",
        "\n",
        "\n",
        "        elif optimizer == 'momentum':\n",
        "            #initialize velocities\n",
        "            if not hasattr(self, 'velocities'):\n",
        "                self.velocities = [np.zeros_like(w) for w in self.weights]\n",
        "            beta1 = 0.9  # momentum parameter\n",
        "            for i in range(len(self.weights)):\n",
        "                self.velocities[i] = beta1 * self.velocities[i] + learning_rate * ((np.dot(activations[i].T, deltas[i]) + weight_decay * self.weights[i]))\n",
        "                self.weights[i] -= self.velocities[i]\n",
        "                self.biases[i] -= learning_rate * np.mean(deltas[i], axis=0)\n",
        "\n",
        "        elif optimizer == 'nesterov':\n",
        "            #initialize velocities\n",
        "            if not hasattr(self, 'velocities'):\n",
        "                self.velocities = [np.zeros_like(w) for w in self.weights]\n",
        "            beta1 = 0.9  # momentum parameter\n",
        "            for i in range(len(self.weights)):\n",
        "                lookahead_weights = self.weights[i] - beta1 * self.velocities[i]\n",
        "                lookahead_biases = self.biases[i] - beta1 * np.mean(self.velocities[i], axis=0)\n",
        "                self.velocities[i] = beta1 * self.velocities[i] + learning_rate * ((np.dot(activations[i].T, deltas[i]) + weight_decay * lookahead_weights))\n",
        "                self.weights[i] -= self.velocities[i]\n",
        "                self.biases[i] -= learning_rate * np.mean(deltas[i], axis=0)\n",
        "\n",
        "\n",
        "        elif optimizer == 'rmsprop':\n",
        "          #initialize rmsprop parameters\n",
        "          if not hasattr(self, 'rmsprop_cache_w'):\n",
        "            self.rmsprop_cache_w = [np.zeros_like(w) for w in self.weights]\n",
        "          if not hasattr(self, 'rmsprop_cache_b'):\n",
        "            self.rmsprop_cache_b = [np.zeros_like(b) for b in self.biases]\n",
        "          for i in range(len(self.weights)):\n",
        "            self.rmsprop_cache_w[i] = beta1 * self.rmsprop_cache_w[i] + (1 - beta1) * (np.dot(activations[i].T, deltas[i])**2 )\n",
        "            self.rmsprop_cache_b[i] = beta1 * self.rmsprop_cache_b[i] + (1 - beta1) * np.mean(deltas[i], axis=0)**2\n",
        "            #check for very small or zero values in rmsprop_cache_w[i]\n",
        "            self.rmsprop_cache_w[i][np.abs(self.rmsprop_cache_w[i]) < epsilon] = epsilon\n",
        "            self.weights[i] -= (learning_rate/ (np.sqrt(self.rmsprop_cache_w[i]) + epsilon))*np.dot(activations[i].T, deltas[i]) + learning_rate * weight_decay * self.weights[i]\n",
        "            self.biases[i] -= learning_rate * np.mean(deltas[i], axis=0)  / (np.sqrt(self.rmsprop_cache_b[i]) + epsilon)\n",
        "\n",
        "        elif optimizer == 'adam':\n",
        "          #initialize adam parameters\n",
        "          if not hasattr(self, 'adam_m_w'):\n",
        "            self.adam_m_w = [np.zeros_like(w) for w in self.weights]\n",
        "          if not hasattr(self, 'adam_m_b'):\n",
        "            self.adam_m_b = [np.zeros_like(b) for b in self.biases]\n",
        "          if not hasattr(self, 'adam_v_w'):\n",
        "            self.adam_v_w = [np.zeros_like(w) for w in self.weights]\n",
        "          if not hasattr(self, 'adam_m_b'):\n",
        "            self.adam_v_b = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "          for i in range(len(self.weights)):\n",
        "            self.adam_m_w[i] = beta1 * self.adam_m_w[i] + (1 - beta1) * np.dot(activations[i].T, deltas[i])\n",
        "            self.adam_m_b[i] = beta1 * self.adam_m_b[i] + (1 - beta1) * np.mean(deltas[i], axis=0)\n",
        "            self.adam_v_w[i] = beta2 * self.adam_v_w[i] + (1 - beta2) * (np.dot(activations[i].T, deltas[i]))**2\n",
        "            self.adam_v_b[i] = beta2 * self.adam_v_b[i] + (1 - beta2) * np.mean(deltas[i], axis=0)**2\n",
        "            m_w_hat = self.adam_m_w[i] / (1 - beta1**(i+1))\n",
        "            m_b_hat = self.adam_m_b[i] / (1 - beta1**(i+1))\n",
        "            v_w_hat = self.adam_v_w[i] / (1 - beta2**(i+1))\n",
        "            v_b_hat = self.adam_v_b[i] / (1 - beta2**(i+1))\n",
        "            self.weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + epsilon) + learning_rate * weight_decay * self.weights[i]\n",
        "            self.biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n",
        "\n",
        "        elif optimizer == 'nadam':\n",
        "\n",
        "          #initialize adam parameters\n",
        "          if not hasattr(self, 'adam_m_w'):\n",
        "            self.adam_m_w = [np.zeros_like(w) for w in self.weights]\n",
        "          if not hasattr(self, 'adam_m_b'):\n",
        "            self.adam_m_b = [np.zeros_like(b) for b in self.biases]\n",
        "          if not hasattr(self, 'adam_v_w'):\n",
        "            self.adam_v_w = [np.zeros_like(w) for w in self.weights]\n",
        "          if not hasattr(self, 'adam_m_b'):\n",
        "            self.adam_v_b = [np.zeros_like(b) for b in self.biases]\n",
        "          for i in range(len(self.weights)):\n",
        "            self.adam_m_w[i] = beta1 * self.adam_m_w[i] + (1 - beta1) * np.dot(activations[i].T, deltas[i])\n",
        "            self.adam_m_b[i] = beta1 * self.adam_m_b[i] + (1 - beta1) * np.mean(deltas[i], axis=0)\n",
        "            self.adam_v_w[i] = beta2 * self.adam_v_w[i] + (1 - beta2) * (np.dot(activations[i].T, deltas[i]))**2\n",
        "            self.adam_v_b[i] = beta2 * self.adam_v_b[i] + (1 - beta2) * np.mean(deltas[i], axis=0)**2\n",
        "            m_w_hat = self.adam_m_w[i] / (1 - beta1**(i+1))\n",
        "            m_b_hat = self.adam_m_b[i] / (1 - beta1**(i+1))\n",
        "            v_w_hat = self.adam_v_w[i] / (1 - beta2**(i+1))\n",
        "            v_b_hat = self.adam_v_b[i] / (1 - beta2**(i+1))\n",
        "            m_w_bar = beta1*m_w_hat + (1 - beta1)*np.dot(activations[i].T, deltas[i])\n",
        "            m_b_bar = beta1*m_b_hat + (1 - beta1)*np.mean(deltas[i], axis=0)\n",
        "            self.weights[i] -= learning_rate * m_w_bar / (np.sqrt(v_w_hat) + epsilon) + learning_rate * weight_decay * self.weights[i]\n",
        "            self.biases[i] -= learning_rate * m_b_bar / (np.sqrt(v_b_hat) + epsilon)\n",
        "\n",
        "\n",
        "\n",
        "    def train(self, X_train, y_train, X_test, y_test_one_hot,act_fun,optimizer,loss_fun, epochs, batch_size, learning_rate, weight_decay):\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(0, len(X_train), batch_size):\n",
        "                X_batch = X_train[i:i+batch_size]\n",
        "                y_batch = y_train[i:i+batch_size]\n",
        "                self.update(X_batch, y_batch, act_fun,optimizer, learning_rate, weight_decay,beta1 = 0.9)\n",
        "\n",
        "\n",
        "            train_loss, train_accuracy = self.evaluate(X_train, y_train,act_fun, weight_decay,loss_fun)\n",
        "            wandb.log({\"train_accuracy\": train_accuracy*100})\n",
        "            wandb.log({\"train_loss\": train_loss})\n",
        "            val_loss, val_accuracy = self.evaluate(X_test, y_test_one_hot,act_fun, weight_decay,loss_fun)\n",
        "            wandb.log({\"val_accuracy\": val_accuracy*100})\n",
        "            wandb.log({\"val_loss\": val_loss})\n",
        "\n",
        "        #return train_loss,train_accuracy\n",
        "\n",
        "    def test(self, X_test, act_fun):\n",
        "        return self.forward(X_test, act_fun)[-1]\n",
        "\n",
        "'''\n",
        "def compute_accuracy(predictions, true_labels):\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "    true_labels = np.argmax(true_labels, axis=1)\n",
        "    accuracy = np.mean(predicted_labels == true_labels)\n",
        "    return accuracy\n",
        "'''\n",
        "\n",
        "#fashion-mnist dataset\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "#preprocessing the data\n",
        "X_train = X_train.reshape(X_train.shape[0], -1) / 255.0\n",
        "X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
        "\n",
        "#convert labels to one-hot encoding\n",
        "num_classes = 10\n",
        "y_train_one_hot = np.eye(num_classes)[y_train]\n",
        "y_test_one_hot = np.eye(num_classes)[y_test]\n",
        "\n",
        "#define neural network\n",
        "input_size = X_train.shape[1]\n",
        "hidden_layers = 3\n",
        "hidden_sizes = [128,64,32]\n",
        "output_size = num_classes\n",
        "\n",
        "#initialize neural network\n",
        "#model = NeuralNetwork(input_size, hidden_layers, hidden_sizes, output_size, int_method='random')\n",
        "\n",
        "#train the model\n",
        "act_fun = 'tanh'  #change activation function here\n",
        "epochs = 2\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0005  #change weight decay here\n",
        "batch_size = 16  #choose batch sizes here\n",
        "optimizer = 'nadam'\n",
        "\n",
        "#train_loss,train_accuracy=model.train(X_train, y_train_one_hot, act_fun,optimizer, epochs, batch_size, learning_rate, weight_decay)\n",
        "#print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "#test the model\n",
        "#predictions = model.test(X_test, act_fun)\n",
        "#accuracy = NeuralNetwork.evaluate(X_test,predictions, y_test_one_hot)\n",
        "#print(\"Accuracy on test set with batch size :\", accuracy * 100, \"%\")\n",
        "\n",
        "#evaluate on test data\n",
        "#test_loss, test_accuracy = model.evaluate(X_test, y_test_one_hot,act_fun,weight_decay)\n",
        "#print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "#References:\n",
        "#1.https://cs229.stanford.edu/main_notes.pdf\n",
        "#2.http://www.cse.iitm.ac.in/~miteshk/CS6910.html\n",
        "#3.https://visualstudiomagazine.com/Articles/2017/06/01/Back-Propagation.aspx?Page=2\n",
        "#4.https://medium.com/@ipylypenko/exploring-neural-networks-with-fashion-mnist-b0a8214b7b7b\n",
        "#5.https://www.youtube.com/watch?v=Wo5dMEP_BbI&list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3\n",
        "#7.https://www.youtube.com/watch?v=LQvRhQwDOm0\n",
        "#8.https://towardsdatascience.com/implementing-different-activation-functions-and-weight-initialization-methods-using-python-c78643b9f20f\n",
        "#9.https://medium.com/konvergen/modifying-adam-to-use-nesterov-accelerated-gradients-nesterov-accelerated-adaptive-moment-67154177e1fd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISiWAKeCZrG7"
      },
      "outputs": [],
      "source": [
        "#Q4,5,6\n",
        "def main():\n",
        "    with wandb.init() as run:\n",
        "        config = wandb.config\n",
        "\n",
        "\n",
        "        hidden_sizes = [config.hidden_sizes] if isinstance(config.hidden_sizes, int) else config.hidden_sizes\n",
        "\n",
        "        model = NeuralNetwork(input_size, config.hidden_layers, config.hidden_sizes, output_size, config.int_method)\n",
        "\n",
        "\n",
        "        model.train(X_train, y_train_one_hot, X_test, y_test_one_hot,config.activation,config.optimizer,config.loss_fun, config.epochs, config.batch_size, config.learning_rate, config.weight_decay)\n",
        "\n",
        "\n",
        "        model.evaluate(X_test, y_test_one_hot,config.activation,config.weight_decay,config.loss_fun)\n",
        "\n",
        "\n",
        "\n",
        "#define the sweep configuration\n",
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'name': 'sweep_cross_entropy',\n",
        "    'metric': {\n",
        "        'name': 'val_accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {'values': [5, 10]},\n",
        "        'hidden_layers': {'values':[3, 4, 5]},\n",
        "        'hidden_sizes': {'values': [32, 64, 128]},\n",
        "        'activation': {'values': ['sigmoid', 'relu', 'tanh']},\n",
        "        'loss_fun': {'values': ['cross_entropy']},\n",
        "        'weight_decay': {'values': [0,0.0005,0.5]},\n",
        "        'learning_rate': {'values': [0.0001, 0.00001]},\n",
        "        'optimizer': {'values': ['sgd','momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']},\n",
        "        'batch_size': {'values': [16, 32, 64]},\n",
        "        'int_method':{'values':['radnom','xavier']}\n",
        "    }\n",
        "}\n",
        "\n",
        "#the sweep\n",
        "sweep_id = wandb.sweep(sweep=sweep_config, project='DLassignment1')\n",
        "\n",
        "#run the sweep agent\n",
        "wandb.agent(sweep_id, function=main,count=150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZHLw-0ZZrG7"
      },
      "outputs": [],
      "source": [
        "#Q7 confusion matrix for best hyperparameters for validation accuracy of 83.91%\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import wandb\n",
        "\n",
        "# Load fashion-mnist dataset\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "X_train = X_train.reshape(X_train.shape[0], -1) / 255.0\n",
        "X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "num_classes = 10\n",
        "y_train_one_hot = np.eye(num_classes)[y_train]\n",
        "y_test_one_hot = np.eye(num_classes)[y_test]\n",
        "\n",
        "# Best hyperparameters\n",
        "best_epochs = 10\n",
        "best_hidden_layers = 4\n",
        "best_hidden_size = 128\n",
        "best_weight_decay = 0.5\n",
        "best_learning_rate = 0.0001\n",
        "best_batch_size = 16\n",
        "best_act_fun = 'tanh'\n",
        "best_loss_fun = 'cross_entropy'\n",
        "best_weight_init = 'random'\n",
        "best_optimizer = 'rmsprop'\n",
        "\n",
        "#the model with best hyperparameters\n",
        "model = NeuralNetwork(input_size=X_train.shape[1],\n",
        "                      hidden_layers=best_hidden_layers,\n",
        "                      hidden_sizes=best_hidden_size,\n",
        "                      output_size=num_classes,\n",
        "                      int_method=best_weight_init)\n",
        "\n",
        "#initialize W&B run\n",
        "wandb.init(project='DLassignment1', config={\n",
        "    'epochs': best_epochs,\n",
        "    'hidden_layers': best_hidden_layers,\n",
        "    'hidden_size': best_hidden_size,\n",
        "    'weight_decay': best_weight_decay,\n",
        "    'learning_rate': best_learning_rate,\n",
        "    'batch_size': best_batch_size,\n",
        "    'activation_function': best_act_fun,\n",
        "    'loss_function': best_loss_fun,\n",
        "    'weight_initialization': best_weight_init,\n",
        "    'optimizer': best_optimizer\n",
        "})\n",
        "\n",
        "#train the model\n",
        "model.train(X_train, y_train_one_hot, X_test, y_test_one_hot,\n",
        "            best_act_fun, best_optimizer, best_loss_fun,\n",
        "            best_epochs, best_batch_size, best_learning_rate,\n",
        "            best_weight_decay)\n",
        "\n",
        "#evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test_one_hot, best_act_fun, best_weight_decay, best_loss_fun)\n",
        "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
        "\n",
        "#generate predictions on the test set\n",
        "predictions = model.test(X_test, best_act_fun)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "#calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
        "\n",
        "img  = pl.matshow(conf_matrix)\n",
        "pl.title('Confusion matrix')\n",
        "pl.colorbar()\n",
        "pl.show()\n",
        "\n",
        "wandb.log({\"confusion matrix\": [ wandb.Image(img, caption='confusion matrix') ]})\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qshj9G3MZrG7"
      },
      "outputs": [],
      "source": [
        "#Q8. In all the models above we used cross entropy loss. Now to compare the cross entropy loss with the squared error loss.\n",
        "def main():\n",
        "    with wandb.init() as run:\n",
        "        config = wandb.config\n",
        "\n",
        "\n",
        "        hidden_sizes = [config.hidden_sizes] if isinstance(config.hidden_sizes, int) else config.hidden_sizes\n",
        "\n",
        "        model = NeuralNetwork(input_size, config.hidden_layers, config.hidden_sizes, output_size, config.int_method)\n",
        "\n",
        "\n",
        "        model.train(X_train, y_train_one_hot, X_test, y_test_one_hot,config.activation,config.optimizer,config.loss_fun, config.epochs, config.batch_size, config.learning_rate, config.weight_decay)\n",
        "\n",
        "\n",
        "        model.evaluate(X_test, y_test_one_hot,config.activation,config.weight_decay,config.loss_fun)\n",
        "\n",
        "\n",
        "\n",
        "#define the sweep configuration\n",
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'name': 'sweep_MSE',\n",
        "    'metric': {\n",
        "        'name': 'val_accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {'values': [5, 10]},\n",
        "        'hidden_layers': {'values':[3, 4, 5]},\n",
        "        'hidden_sizes': {'values': [32, 64, 128]},\n",
        "        'activation': {'values': ['sigmoid', 'relu', 'tanh']},\n",
        "        'loss_fun': {'values': ['MSE']},\n",
        "        'weight_decay': {'values': [0,0.0005,0.5]},\n",
        "        'learning_rate': {'values': [0.0001, 0.00001]},\n",
        "        'optimizer': {'values': ['sgd','momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']},\n",
        "        'batch_size': {'values': [16, 32, 64]},\n",
        "        'int_method':{'values':['radnom','xavier']}\n",
        "    }\n",
        "}\n",
        "\n",
        "#the sweep\n",
        "sweep_id = wandb.sweep(sweep=sweep_config, project='DLassignment1')\n",
        "\n",
        "wandb.agent(sweep_id, function=main,count=150)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q.10\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "\n",
        "#MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], -1) / 255.0\n",
        "X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
        "\n",
        "\n",
        "num_classes = 10\n",
        "y_train_one_hot = np.eye(num_classes)[y_train]\n",
        "y_test_one_hot = np.eye(num_classes)[y_test]\n",
        "\n",
        "#best three configuration for mnist dataset\n",
        "configs = [\n",
        "    {\"epochs\": 10, \"hidden_layers\": 4, \"hidden_sizes\": 128, \"activation\": \"tanh\", \"optimizer\": \"rmsprop\",\n",
        "     \"learning_rate\": 0.0001, \"weight_decay\": 0.5, \"batch_size\": 16, \"loss_fun\": \"cross_entropy\",\"int_method\":'random'},\n",
        "    {\"epochs\": 5, \"hidden_layers\": 5, \"hidden_sizes\": 64, \"activation\": \"tanh\", \"optimizer\": \"momentum\",\n",
        "     \"learning_rate\": 0.0001, \"weight_decay\": 0.5, \"batch_size\": 32, \"loss_fun\": \"cross_entropy\",\"int_method\":'xavier'},\n",
        "    {\"epochs\": 5, \"hidden_layers\": 3, \"hidden_sizes\": 64, \"activation\": \"relu\", \"optimizer\": \"momentum\",\n",
        "     \"learning_rate\": 0.0001, \"weight_decay\": 0.5, \"batch_size\": 64, \"loss_fun\": \"cross_entropy\",\"int_method\":'xavier'},\n",
        "]\n",
        "\n",
        "for i, config in enumerate(configs, start=1):\n",
        "    model = NeuralNetwork(input_size=X_train.shape[1],\n",
        "                          hidden_layers=config[\"hidden_layers\"],\n",
        "                          hidden_sizes=config[\"hidden_sizes\"],\n",
        "                          output_size=num_classes,\n",
        "                          int_method=config[\"int_method\"])\n",
        "    model.train(X_train, y_train_one_hot, X_test, y_test_one_hot,\n",
        "                config[\"activation\"], config[\"optimizer\"], config[\"loss_fun\"],\n",
        "                config[\"epochs\"], config[\"batch_size\"],\n",
        "                learning_rate=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test_one_hot,\n",
        "                                              act_fun=config[\"activation\"],\n",
        "                                              weight_decay=config[\"weight_decay\"],\n",
        "                                              loss_fun=config[\"loss_fun\"])\n",
        "    print(f\"Configuration {i}: Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "i9bp5R-XZrG7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}