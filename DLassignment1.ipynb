{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM50IZK/mukyy/GhfVjLFTV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manish2021iitd/Deep-Learning/blob/main/DLassignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q1\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "# Load Fashion-MNIST dataset\n",
        "(training_images, training_labels), (testing_images,testing_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Plot one sample image for each class\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.imshow(training_images[training_labels == i][0], cmap='gray')\n",
        "    plt.title(class_names[i])\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "peru7mjbMqTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2\n",
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        self.input_size = input_size #number of inputs nodes\n",
        "        self.hidden_sizes = hidden_sizes #list of neurons in each hidden layer\n",
        "        self.output_size = output_size #number of output neurons in output layer\n",
        "        self.weights = [] #list of weight matrices for each layer\n",
        "        self.biases = []  #list of bias vectors for each hidden layer and output layer\n",
        "\n",
        "        #innitializing the weights and biases for each layer\n",
        "        sizes = [input_size] + hidden_sizes + [output_size] #using list concatination to make a list of number of nodes in each layer\n",
        "        for i in range(len(sizes) - 1):\n",
        "            self.weights.append(np.random.randn(sizes[i+1], sizes[i]))\n",
        "            self.biases.append(np.random.randn(sizes[i+1]))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_scores = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "      # Forward pass\n",
        "      activations = [X]\n",
        "      for i in range(len(self.weights)):\n",
        "        z = np.dot(activations[-1], self.weights[i].T) + self.biases[i]  # Transpose weight matrix\n",
        "        a = self.sigmoid(z) if i < len(self.weights) - 1 else self.softmax(z)\n",
        "        activations.append(a)\n",
        "      return activations\n",
        "\n",
        "    def predict(self, X):\n",
        "        #predict output probabilities\n",
        "        activations = self.forward(X)\n",
        "        return activations[-1]\n",
        "\n",
        "#load fashion-mnist dataset\n",
        "from keras.datasets import fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "#preprocess the data\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "#define neural network\n",
        "input_size = X_train.shape[1]  #28x28 images flattened\n",
        "hidden_sizes = [128, 64]  #size of hidden layers\n",
        "output_size = 10  # 10 classes in fashion-mnist\n",
        "\n",
        "#initialize neural network\n",
        "model = NeuralNetwork(input_size, hidden_sizes, output_size)\n",
        "\n",
        "#predict probabilities for the test set\n",
        "probabilities = model.predict(X_test)\n",
        "\n",
        "#print the output probabilities\n",
        "print(\"Output Probabilities:\", probabilities)"
      ],
      "metadata": {
        "id": "XaWViZYXMzya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.output_size = output_size\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        #initialize weights and biases for each layer\n",
        "        sizes = [input_size] + hidden_sizes + [output_size]\n",
        "        for i in range(len(sizes) - 1):\n",
        "            self.weights.append(np.random.randn(sizes[i+1], sizes[i]))\n",
        "            self.biases.append(np.random.randn(sizes[i+1],1))\n",
        "\n",
        "        #initialize velocities for momentum-based optimization\n",
        "        self.velocities = [np.zeros_like(w) for w in self.weights]\n",
        "\n",
        "        #initialize rmsprop parameters\n",
        "        self.rmsprop_cache_w = [np.zeros_like(w) for w in self.weights]\n",
        "        self.rmsprop_cache_b = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "        #initialize adam parameters\n",
        "        self.adam_m_w = [np.zeros_like(w) for w in self.weights]\n",
        "        self.adam_m_b = [np.zeros_like(b) for b in self.biases]\n",
        "        self.adam_v_w = [np.zeros_like(w) for w in self.weights]\n",
        "        self.adam_v_b = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "      return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def d_sigmoid(self,x):\n",
        "      return self.sigmoid(x)*(1-self.sigmoid(x))\n",
        "\n",
        "    def softmax(self, x):\n",
        "      exp_scores = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "      return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "    def logloss(self, y, y_label):\n",
        "      error = -(y_label*np.log(y+1e-10)+(1-y_label)*np.log(1-y+1e-10))\n",
        "      return np.sum(error, axis = 0)/error.shape[0]\n",
        "\n",
        "    def Cross_entropy(self, y, y_hat):\n",
        "      da = (1-y)/(1-y_hat) - (y)/(y_hat+1e-10)\n",
        "      epoch_loss = self.__logloss(y_hat, y)\n",
        "      return da, epoch_loss\n",
        "\n",
        "    #forwardpropagation\n",
        "    def forward(self, X):\n",
        "      activations = [X] #create a list of all activations\n",
        "      for i in range(len(self.weights)):\n",
        "        z = np.dot(self.weights[i],activations[-1].T) + self.biases[i]  #preactivation\n",
        "        a = self.sigmoid(z) if i < len(self.weights) - 1 else self.softmax(z) #activation/output\n",
        "        activations.append(a) #appending the activations list\n",
        "      return activations\n",
        "\n",
        "    #backwardpropagation\n",
        "    def backward(self, y, activations):\n",
        "        delta = activations[-1] - y\n",
        "        for i in range(len(self.weights) - 1, -1, -1):\n",
        "            dz = delta * (activations[i+1] * (1 - activations[i+1])) if i < len(self.weights) - 1 else delta\n",
        "            dw = np.dot(dz.T, activations[i])\n",
        "            db = np.sum(dz, axis=0)\n",
        "            delta = np.dot(dz, self.weights[i])\n",
        "        return dw,db\n",
        "\n",
        "\n",
        "    def update(self, X, y, optimizer, epochs, learning_rate=0.0001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "      for _ in range(epochs):\n",
        "        activations = self.forward(X)\n",
        "        dw, db = self.backward(y, activations)\n",
        "\n",
        "        if optimizer == 'sgd':\n",
        "          for i in range(len(self.weights)):\n",
        "            self.weights[i] -= learning_rate * dw[i]\n",
        "            self.biases[i] -= learning_rate * db[i]\n",
        "\n",
        "        elif optimizer == 'momentum':\n",
        "          for i in range(len(self.weights)):\n",
        "            self.velocities[i] = beta1 * self.velocities[i] + (1 - beta1) * dw[i]\n",
        "            self.weights[i] -= learning_rate * self.velocities[i]\n",
        "            self.biases[i] -= learning_rate * db[i]\n",
        "\n",
        "        elif optimizer == 'nesterov':\n",
        "          lookahead_weights = [w - beta1 * v for w, v in zip(self.weights, self.velocities)]\n",
        "          lookahead_biases = [b - beta1 * v[-1] for b, v in zip(self.biases, self.velocities)]\n",
        "          lookahead_activations = self.forward(X)\n",
        "          lookahead_dw, lookahead_db = self.backward(y, lookahead_activations)\n",
        "          for i in range(len(self.weights)):\n",
        "            self.velocities[i] = beta1 * self.velocities[i] + (1 - beta1) * lookahead_dw[i]\n",
        "            self.weights[i] -= learning_rate * self.velocities[i]\n",
        "            self.biases[i] -= learning_rate * lookahead_db[i]\n",
        "\n",
        "        elif optimizer == 'rmsprop':\n",
        "          for i in range(len(self.weights)):\n",
        "            self.rmsprop_cache_w[i] = beta1 * self.rmsprop_cache_w[i] + (1 - beta1) * dw[i]**2\n",
        "            self.rmsprop_cache_b[i] = beta1 * self.rmsprop_cache_b[i] + (1 - beta1) * db[i]**2\n",
        "            self.weights[i] -= learning_rate * dw[i] / (np.sqrt(self.rmsprop_cache_w[i]) + epsilon)\n",
        "            self.biases[i] -= learning_rate * db[i] / (np.sqrt(self.rmsprop_cache_b[i]) + epsilon)\n",
        "\n",
        "        elif optimizer == 'adam':\n",
        "          for i in range(len(self.weights)):\n",
        "            self.adam_m_w[i] = beta1 * self.adam_m_w[i] + (1 - beta1) * dw[i]\n",
        "            self.adam_m_b[i] = beta1 * self.adam_m_b[i] + (1 - beta1) * db[i]\n",
        "            self.adam_v_w[i] = beta2 * self.adam_v_w[i] + (1 - beta2) * dw[i]**2\n",
        "            self.adam_v_b[i] = beta2 * self.adam_v_b[i] + (1 - beta2) * db[i]**2\n",
        "            m_w_hat = self.adam_m_w[i] / (1 - beta1**(i+1))\n",
        "            m_b_hat = self.adam_m_b[i] / (1 - beta1**(i+1))\n",
        "            v_w_hat = self.adam_v_w[i] / (1 - beta2**(i+1))\n",
        "            v_b_hat = self.adam_v_b[i] / (1 - beta2**(i+1))\n",
        "            self.weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + epsilon)\n",
        "            self.biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n",
        "\n",
        "        elif optimizer == 'nadam':\n",
        "          for i in range(len(self.weights)):\n",
        "            self.adam_m_w[i] = beta1 * self.adam_m_w[i] + (1 - beta1) * dw[i]\n",
        "            self.adam_m_b[i] = beta1 * self.adam_m_b[i] + (1 - beta1) * db[i]\n",
        "            self.adam_v_w[i] = beta2 * self.adam_v_w[i] + (1 - beta2) * dw[i]**2\n",
        "            self.adam_v_b[i] = beta2 * self.adam_v_b[i] + (1 - beta2) * db[i]**2\n",
        "            m_w_hat = self.adam_m_w[i] / (1 - beta1**(i+1))\n",
        "            m_b_hat = self.adam_m_b[i] / (1 - beta1**(i+1))\n",
        "            v_w_hat = self.adam_v_w[i] / (1 - beta2**(i+1))\n",
        "            v_b_hat = self.adam_v_b[i] / (1 - beta2**(i+1))\n",
        "            self.weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + epsilon)\n",
        "            self.biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n",
        "\n",
        "\n",
        "    def train(self, X, y, optimizer, epochs, batch_size):\n",
        "      #train the neural network using different optimizers\n",
        "      for epoch in range(epochs):\n",
        "        for i in range(0, len(X), batch_size):\n",
        "          X_batch = X[i:i+batch_size]\n",
        "          y_batch = y[i:i+batch_size]\n",
        "          self.update(X_batch,y_batch,optimizer,epochs)\n",
        "\n",
        "    def test(self, X):\n",
        "      activations = self.forward(X)\n",
        "      return activations[-1]\n",
        "\n",
        "\n",
        "#load fashion-mnist dataset\n",
        "from keras.datasets import fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "#preprocess the data\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "print(X_train.shape)\n",
        "#define neural network\n",
        "input_size = 784#size of inputs\n",
        "hidden_sizes = [128, 64]  #size of neurons in each hidden layers\n",
        "output_size = 10  #taking 10 output neurons as we have classify 10 classes in fashion-mnist\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "num_classes = 10\n",
        "y_train_one_hot = np.eye(num_classes)[y_train]\n",
        "y_test_one_hot = np.eye(num_classes)[y_test]\n",
        "\n",
        "#initialize neural network\n",
        "model = NeuralNetwork(input_size, hidden_sizes, output_size)\n",
        "\n",
        "#please choose optimizer = 'sgd'/'momentum'/'nesterov'/'rmsprop'/'adam'/'nadam'\n",
        "#and number of epochs and batch size here\n",
        "model.train(X_train, y_train_one_hot,'nadam', epochs=10, batch_size=32)\n",
        "\n",
        "results = model.test(X_test)\n",
        "\n",
        "results = np.argmax(results, axis=-1)\n",
        "real_labels = np.argmax(y_test, axis=-1)\n",
        "accuracy = sum(1*(results==real_labels))/len(results)\n",
        "print(\"The accuracy of testing is \", accuracy*100, \"%\")\n",
        "#References:\n",
        "#1.https://cs229.stanford.edu/main_notes.pdf\n",
        "#2.http://www.cse.iitm.ac.in/~miteshk/CS6910.html\n",
        "#3.https://visualstudiomagazine.com/Articles/2017/06/01/Back-Propagation.aspx?Page=2\n",
        "#4.https://medium.com/@ipylypenko/exploring-neural-networks-with-fashion-mnist-b0a8214b7b7b\n",
        "#5.https://www.youtube.com/watch?v=Wo5dMEP_BbI&list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3\n",
        "#7.https://www.youtube.com/watch?v=LQvRhQwDOm0"
      ],
      "metadata": {
        "id": "Nm7URO24NPHS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}